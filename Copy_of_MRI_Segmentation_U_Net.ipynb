{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MRI_Segmentation_U-Net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FalseFeint/MertsMinions/blob/main/Copy_of_MRI_Segmentation_U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fekwqJEDtat",
        "outputId": "3de048ec-1102-43bc-9e24-38360611ca46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#                                 _     ___                        \n",
        "#    __    __ _   _ __    ___    | |   / __|  __ _    ___    ___   \n",
        "#   / _|  / _` | | '  \\  / -_)   | |  | (__  / _` |  (_-<   / -_)  \n",
        "#   \\__|_ \\__,_| |_|_|_| \\___|  _|_|_  \\___| \\__,_|  /__/_  \\___|  \n",
        "# _|\"\"\"\"\"_|\"\"\"\"\"_|\"\"\"\"\"_|\"\"\"\"\"_|\"\"\"\"\"_|\"\"\"\"\"_|\"\"\"\"\"_|\"\"\"\"\"_|\"\"\"\"\"| \n",
        "# \"`-0-0-\"`-0-0-\"`-0-0-\"`-0-0-\"`-0-0-\"`-0-0-\"`-0-0-\"`-0-0-\"`-0-0-' \n",
        "\n",
        "\"\"\"\n",
        "** camelCase!!!!!! **\n",
        "\n",
        "i = int\n",
        "dbl = double\n",
        "flt = float\n",
        "ten = tensor\n",
        "arr = array\n",
        "iArr = int array\n",
        "char = character\n",
        "str = string\n",
        "dict = dictionary\n",
        "lst = list\n",
        "mat = Matrix\n",
        "im = Image\n",
        "\"\"\"\n",
        "#   _____                              _____        _                 _   \n",
        "#  |  __ \\                            |  __ \\      | |               | |  \n",
        "#  | |__) _ __ ___   ___ ___ ___ ___  | |  | | __ _| |_ __ _ ___  ___| |_ \n",
        "#  |  ___| '__/ _ \\ / __/ _ / __/ __| | |  | |/ _` | __/ _` / __|/ _ | __|\n",
        "#  | |   | | | (_) | (_|  __\\__ \\__ \\ | |__| | (_| | || (_| \\__ |  __| |_ \n",
        "#  |_|   |_|  \\___/ \\___\\___|___|___/ |_____/ \\__,_|\\__\\__,_|___/\\___|\\__|\n",
        "\n",
        "#  _____        _                                                   _        _   _             \n",
        "#  |  __ \\      | |            /\\                                   | |      | | (_)            \n",
        "#  | |  | | __ _| |_ __ _     /  \\  _   _  __ _ _ __ ___   ___ _ __ | |_ __ _| |_ _  ___  _ __  \n",
        "#  | |  | |/ _` | __/ _` |   / /\\ \\| | | |/ _` | '_ ` _ \\ / _ \\ '_ \\| __/ _` | __| |/ _ \\| '_ \\ \n",
        "#  | |__| | (_| | || (_| |  / ____ \\ |_| | (_| | | | | | |  __/ | | | || (_| | |_| | (_) | | | |\n",
        "#  |_____/ \\__,_|\\__\\__,_| /_/    \\_\\__,_|\\__, |_| |_| |_|\\___|_| |_|\\__\\__,_|\\__|_|\\___/|_| |_|\n",
        "#                                         __/ |                                                \n",
        "#                                        |___/                                                 \n",
        "\n",
        "#   _______        _       _             \n",
        "#  |__   __|      (_)     (_)            \n",
        "#     | |_ __ __ _ _ _ __  _ _ __   __ _ \n",
        "#     | | '__/ _` | | '_ \\| | '_ \\ / _` |\n",
        "#     | | | | (_| | | | | | | | | | (_| |\n",
        "#     |_|_|  \\__,_|_|_| |_|_|_| |_|\\__, |\n",
        "#                                   __/ |\n",
        "#                                  |___/ \n",
        "\n",
        "\"\"\"\n",
        "TA: Can we follow an example closely (especially for baseline given that we \n",
        "are noobs)\n",
        "What should we include in the report?\n",
        "What are your thoughts on skullstripping?\n",
        "What are ways we can be innovative with our implementation?\n",
        "Any advice? Anything we should watch out for?\n",
        "How can we use the GPU in google colab?\n",
        "\n",
        "Professor said this:\n",
        "the expectation is that you will describe your problem, data and code that performs I/O and pre-processing.\n",
        "You can also offer a visual exploration of the data and show some results with simple baseline method,\n",
        "You can include preliminary results with implemented ML algorithm.\n",
        "\n",
        "The main text should NOT exceed 4 pages.\n",
        "\n",
        "Answers:\n",
        "Modifying UNet - Easily doable.  https://arxiv.org/abs/1603.07285\n",
        "Skull Stripping - Look into resources online to implement this\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTA: Can we follow an example closely (especially for baseline given that we \\nare noobs)\\nWhat should we include in the report?\\nWhat are your thoughts on skullstripping?\\nWhat are ways we can be innovative with our implementation?\\nAny advice? Anything we should watch out for?\\nHow can we use the GPU in google colab?\\n\\nProfessor said this:\\nthe expectation is that you will describe your problem, data and code that performs I/O and pre-processing.\\nYou can also offer a visual exploration of the data and show some results with simple baseline method,\\nYou can include preliminary results with implemented ML algorithm.\\n\\nThe main text should NOT exceed 4 pages.\\n\\nAnswers:\\nModifying UNet - Easily doable.  https://arxiv.org/abs/1603.07285\\nSkull Stripping - Look into resources online to implement this\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQPefAMY-9UX"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuqvstzy-rPf"
      },
      "source": [
        "#UNet\n",
        "Create UNet architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76FtFLpw-h3v"
      },
      "source": [
        "#   _    _ _   _      _   \n",
        "#  | |  | | \\ | |    | |  \n",
        "#  | |  | |  \\| | ___| |_ \n",
        "#  | |  | | . ` |/ _ | __|\n",
        "#  | |__| | |\\  |  __| |_ \n",
        "#   \\____/|_| \\_|\\___|\\__|\n",
        "\n",
        "class UNet(nn.Module): \n",
        "  def __init__(self, iInChannel=3, iOutChannel=1, iInFeatures=32):\n",
        "    super(UNet, self).__init__()\n",
        "\n",
        "    # TODO: See if we can reduce this to for-loop using eval\n",
        "    # TODO: Look into initializing all pool layers at once\n",
        "    iFeatures = iInFeatures\n",
        "\n",
        "    # Declare all our down blocks and max pool layers\n",
        "    self.downBlock1 = UNet._block(iInChannels, iFeatures, name=\"downBlock1\")\n",
        "    self.maxPool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.downBlock2 = UNet._block(iFeatures, iFeatures * 2, name=\"downBlock2\")\n",
        "    self.maxPool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.downBlock3 = UNet._block(iFeatures * 2, iFeatures * 4, name=\"downBlock3\")\n",
        "    self.maxPool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.downBlock4 = UNet._block(iFeatures * 4, iFeatures * 8, name=\"downBlock4\")\n",
        "    self.maxPool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    # Declare bottom of UNet\n",
        "    self.bottleneck = UNet._block(iFeatures * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "    # TODO: See if we can reduce this to for-loop using eval\n",
        "    # TODO: Look into initializing all pool layers at once\n",
        "    # Declare up-blocks and up-convolutions\n",
        "    self.upconv4 = nn.ConvTranspose2d(\n",
        "        iFeatures * 16, iFeatures * 8, kernel_size=2, stride=2)\n",
        "    self.upBlock4 = UNet._block((iFeatures * 8) * 2, iFeatures * 8, name=\"upBlock4\")\n",
        "    \n",
        "    self.upconv3 = nn.ConvTranspose2d(\n",
        "        iFeatures * 8, iFeatures * 4, kernel_size=2, stride=2)\n",
        "    self.upBlock3 = UNet._block((iFeatures * 4) * 2, iFeatures * 4, name=\"upBlock3\")\n",
        "    \n",
        "    self.upconv2 = nn.ConvTranspose2d(\n",
        "        iFeatures * 4, iFeatures * 2, kernel_size=2, stride=2)\n",
        "    self.upBlock2 = UNet._block((iFeatures * 2) * 2, iFeatures * 2, name=\"upBlock2\")\n",
        "    \n",
        "    self.upconv1 = nn.ConvTranspose2d(\n",
        "        iFeatures * 2, iFeatures, kernel_size=2, stride=2)\n",
        "    self.upBlock1 = UNet._block(iFeatures * 2, iFeatures, name=\"upBlock1\")\n",
        "\n",
        "    self.conv = nn.Conv2d(\n",
        "        in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "    )\n",
        "\n",
        "    # \n",
        "    def forward(self, x):\n",
        "        # Connect down blocks with max-pool layers\n",
        "        downBlock1 = self.downBlock1(x)\n",
        "        downBlock2 = self.downBlock2(self.maxPool1(downBlock1))\n",
        "        downBlock3 = self.downBlock3(self.maxPool2(downBlock2))\n",
        "        downBlock4 = self.downBlock4(self.maxPool3(downBlock3))\n",
        "\n",
        "        # Add bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool4(downBlock4))\n",
        "\n",
        "        # Connect up blocks with up convolutions and skip connections\n",
        "        upBlock4 = self.upconv4(bottleneck)\n",
        "        upBlock4 = torch.cat((upBlock4, downBlock4), dim=1)\n",
        "        upBlock4 = self.upBlock4(upBlock4)\n",
        "        upBlock3 = self.upconv3(upBlock4)\n",
        "        upBlock3 = torch.cat((upBlock3, downBlock3), dim=1)\n",
        "        upBlock3 = self.upBlock3(upBlock3)\n",
        "        upBlock2 = self.upconv2(upBlock3)\n",
        "        upBlock2 = torch.cat((upBlock2, downBlock2), dim=1)\n",
        "        upBlock2 = self.upBlock2(upBlock2)\n",
        "        upBlock1 = self.upconv1(upBlock2)\n",
        "        upBlock1 = torch.cat((upBlock1, downBlock1), dim=1)\n",
        "        upBlock1 = self.upBlock1(upBlock1)\n",
        "        return torch.sigmoid(self.conv(upBlock1))\n",
        "\n",
        "\n",
        "    \"\"\" \n",
        "    This function is useful for blocking together convolutions, activations, \n",
        "    and max pooling for each \"meta-layer\" in our U-Net architecture \n",
        "\n",
        "    Convolution layers in this block have specified in channels, \n",
        "    features, a kernel size of 3, padding of 1 and no bias\n",
        "\n",
        "    We use batch normalization to reduce internal covariance\n",
        "\n",
        "    ReLU is used for non-linearity activation\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgCwdeWaKh9S",
        "outputId": "89a12c2a-10cc-46e8-d122-74c7491aa90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd ~/../content/drive/My\\ Drive/ML\\ Final\\ Project/MertsMinions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/ML Final Project/MertsMinions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLGnOJ8P_R9Y",
        "outputId": "c407ca55-48dc-48c4-dccf-ab0d15586199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "from tqdm import tqdm\n",
        "from skimage.exposure import rescale_intensity\n",
        "from skimage.io import imread, imsave\n",
        "from skimage.transform import resize, rescale, rotate\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "\n",
        "def crop_sample(x):\n",
        "    volume, mask = x\n",
        "    volume[volume < np.max(volume) * 0.1] = 0\n",
        "    z_projection = np.max(np.max(np.max(volume, axis=-1), axis=-1), axis=-1)\n",
        "    z_nonzero = np.nonzero(z_projection)\n",
        "    z_min = np.min(z_nonzero)\n",
        "    z_max = np.max(z_nonzero) + 1\n",
        "    y_projection = np.max(np.max(np.max(volume, axis=0), axis=-1), axis=-1)\n",
        "    y_nonzero = np.nonzero(y_projection)\n",
        "    y_min = np.min(y_nonzero)\n",
        "    y_max = np.max(y_nonzero) + 1\n",
        "    x_projection = np.max(np.max(np.max(volume, axis=0), axis=0), axis=-1)\n",
        "    x_nonzero = np.nonzero(x_projection)\n",
        "    x_min = np.min(x_nonzero)\n",
        "    x_max = np.max(x_nonzero) + 1\n",
        "    return (\n",
        "        volume[z_min:z_max, y_min:y_max, x_min:x_max],\n",
        "        mask[z_min:z_max, y_min:y_max, x_min:x_max],\n",
        "    )\n",
        "\n",
        "\n",
        "def pad_sample(x):\n",
        "    volume, mask = x\n",
        "    a = volume.shape[1]\n",
        "    b = volume.shape[2]\n",
        "    if a == b:\n",
        "        return volume, mask\n",
        "    diff = (max(a, b) - min(a, b)) / 2.0\n",
        "    if a > b:\n",
        "        padding = ((0, 0), (0, 0), (int(np.floor(diff)), int(np.ceil(diff))))\n",
        "    else:\n",
        "        padding = ((0, 0), (int(np.floor(diff)), int(np.ceil(diff))), (0, 0))\n",
        "    mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
        "    padding = padding + ((0, 0),)\n",
        "    volume = np.pad(volume, padding, mode=\"constant\", constant_values=0)\n",
        "    return volume, mask\n",
        "\n",
        "\n",
        "def resize_sample(x, size=256):\n",
        "    volume, mask = x\n",
        "    v_shape = volume.shape\n",
        "    out_shape = (v_shape[0], size, size)\n",
        "    mask = resize(\n",
        "        mask,\n",
        "        output_shape=out_shape,\n",
        "        order=0,\n",
        "        mode=\"constant\",\n",
        "        cval=0,\n",
        "        anti_aliasing=False,\n",
        "    )\n",
        "    out_shape = out_shape + (v_shape[3],)\n",
        "    volume = resize(\n",
        "        volume,\n",
        "        output_shape=out_shape,\n",
        "        order=2,\n",
        "        mode=\"constant\",\n",
        "        cval=0,\n",
        "        anti_aliasing=False,\n",
        "    )\n",
        "    return volume, mask\n",
        "\n",
        "\n",
        "def normalize_volume(volume):\n",
        "    p10 = np.percentile(volume, 10)\n",
        "    p99 = np.percentile(volume, 99)\n",
        "    volume = rescale_intensity(volume, in_range=(p10, p99))\n",
        "    m = np.mean(volume, axis=(0, 1, 2))\n",
        "    s = np.std(volume, axis=(0, 1, 2))\n",
        "    volume = (volume - m) / s\n",
        "    return volume\n",
        "\n",
        "class BrainSegmentationDataset(Dataset):\n",
        "    \"\"\"Brain MRI dataset for FLAIR abnormality segmentation\"\"\"\n",
        "\n",
        "    in_channels = 3\n",
        "    out_channels = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir,\n",
        "        transform=None,\n",
        "        image_size=256,\n",
        "        subset=\"train\",\n",
        "        random_sampling=True,\n",
        "        seed=42,\n",
        "    ):\n",
        "        assert subset in [\"all\", \"train\", \"validation\"]\n",
        "\n",
        "        # read images\n",
        "        volumes = {}\n",
        "        masks = {}\n",
        "        print(\"reading {} images...\".format(subset))\n",
        "        for (dirpath, dirnames, filenames) in os.walk(images_dir):\n",
        "            image_slices = []\n",
        "            mask_slices = []\n",
        "            for filename in sorted(\n",
        "                filter(lambda f: \".tif\" in f, filenames),\n",
        "                key=lambda x: int(x.split(\".\")[-2].split(\"_\")[4]),\n",
        "            ):\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                if \"mask\" in filename:\n",
        "                    mask_slices.append(imread(filepath, as_gray=True))\n",
        "                else:\n",
        "                    image_slices.append(imread(filepath))\n",
        "            patient_id = \"None\"\n",
        "            if len(image_slices) > 0:\n",
        "                patient_id = dirpath.split(\"/\")[-1]\n",
        "                volumes[patient_id] = np.array(image_slices[1:-1])\n",
        "                masks[patient_id] = np.array(mask_slices[1:-1])\n",
        "            dictIndex = len(volumes)\n",
        "            print(\"Volume: \" + str(dictIndex) \\\n",
        "                  + \" Patient ID: \" + str(patient_id) \\\n",
        "                  + \" Images: \" + str(len(image_slices)) \\\n",
        "                  + \" Masks: \" + str(len(mask_slices)))\n",
        "\n",
        "        self.patients = sorted(volumes)\n",
        "\n",
        "        # select cases to subset\n",
        "        if not subset == \"all\":\n",
        "            random.seed(seed)\n",
        "            validation_patients = random.sample(self.patients, k=10)\n",
        "            if subset == \"validation\":\n",
        "                self.patients = validation_patients\n",
        "            else:\n",
        "                self.patients = sorted(\n",
        "                    list(set(self.patients).difference(validation_patients))\n",
        "                )\n",
        "\n",
        "        print(\"preprocessing {} volumes...\".format(subset))\n",
        "        # create list of tuples (volume, mask)\n",
        "        self.volumes = [(volumes[k], masks[k]) for k in self.patients]\n",
        "\n",
        "        print(\"cropping {} volumes...\".format(subset))\n",
        "        # crop to smallest enclosing volume\n",
        "        self.volumes = [crop_sample(v) for v in self.volumes]\n",
        "\n",
        "        print(\"padding {} volumes...\".format(subset))\n",
        "        # pad to square\n",
        "        self.volumes = [pad_sample(v) for v in self.volumes]\n",
        "\n",
        "        print(\"resizing {} volumes...\".format(subset))\n",
        "        # resize\n",
        "        self.volumes = [resize_sample(v, size=image_size) for v in self.volumes]\n",
        "\n",
        "        print(\"normalizing {} volumes...\".format(subset))\n",
        "        # normalize channel-wise\n",
        "        self.volumes = [(normalize_volume(v), m) for v, m in self.volumes]\n",
        "\n",
        "        # probabilities for sampling slices based on masks\n",
        "        self.slice_weights = [m.sum(axis=-1).sum(axis=-1) for v, m in self.volumes]\n",
        "        self.slice_weights = [\n",
        "            (s + (s.sum() * 0.1 / len(s))) / (s.sum() * 1.1) for s in self.slice_weights\n",
        "        ]\n",
        "\n",
        "        # add channel dimension to masks\n",
        "        self.volumes = [(v, m[..., np.newaxis]) for (v, m) in self.volumes]\n",
        "\n",
        "        print(\"done creating {} dataset\".format(subset))\n",
        "\n",
        "        # create global index for patient and slice (idx -> (p_idx, s_idx))\n",
        "        num_slices = [v.shape[0] for v, m in self.volumes]\n",
        "        self.patient_slice_index = list(\n",
        "            zip(\n",
        "                sum([[i] * num_slices[i] for i in range(len(num_slices))], []),\n",
        "                sum([list(range(x)) for x in num_slices], []),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.random_sampling = random_sampling\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.patient_slice_index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient = self.patient_slice_index[idx][0]\n",
        "        slice_n = self.patient_slice_index[idx][1]\n",
        "\n",
        "        if self.random_sampling:\n",
        "            patient = np.random.randint(len(self.volumes))\n",
        "            slice_n = np.random.choice(\n",
        "                range(self.volumes[patient][0].shape[0]), p=self.slice_weights[patient]\n",
        "            )\n",
        "\n",
        "        v, m = self.volumes[patient]\n",
        "        image = v[slice_n]\n",
        "        mask = m[slice_n]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image, mask = self.transform((image, mask))\n",
        "\n",
        "        # fix dimensions (C, H, W)\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        mask = mask.transpose(2, 0, 1)\n",
        "\n",
        "        image_tensor = torch.from_numpy(image.astype(np.float32))\n",
        "        mask_tensor = torch.from_numpy(mask.astype(np.float32))\n",
        "\n",
        "        # return tensors\n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "def transforms(scale=None, angle=None, flip_prob=None):\n",
        "    transform_list = []\n",
        "\n",
        "    if scale is not None:\n",
        "        transform_list.append(Scale(scale))\n",
        "    if angle is not None:\n",
        "        transform_list.append(Rotate(angle))\n",
        "    if flip_prob is not None:\n",
        "        transform_list.append(HorizontalFlip(flip_prob))\n",
        "\n",
        "    return Compose(transform_list)\n",
        "\n",
        "\n",
        "class Scale(object):\n",
        "\n",
        "    def __init__(self, scale):\n",
        "        self.scale = scale\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample\n",
        "\n",
        "        img_size = image.shape[0]\n",
        "\n",
        "        scale = np.random.uniform(low=1.0 - self.scale, high=1.0 + self.scale)\n",
        "\n",
        "        image = rescale(\n",
        "            image,\n",
        "            (scale, scale),\n",
        "            multichannel=True,\n",
        "            preserve_range=True,\n",
        "            mode=\"constant\",\n",
        "            anti_aliasing=False,\n",
        "        )\n",
        "        mask = rescale(\n",
        "            mask,\n",
        "            (scale, scale),\n",
        "            order=0,\n",
        "            multichannel=True,\n",
        "            preserve_range=True,\n",
        "            mode=\"constant\",\n",
        "            anti_aliasing=False,\n",
        "        )\n",
        "\n",
        "        if scale < 1.0:\n",
        "            diff = (img_size - image.shape[0]) / 2.0\n",
        "            padding = ((int(np.floor(diff)), int(np.ceil(diff))),) * 2 + ((0, 0),)\n",
        "            image = np.pad(image, padding, mode=\"constant\", constant_values=0)\n",
        "            mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n",
        "        else:\n",
        "            x_min = (image.shape[0] - img_size) // 2\n",
        "            x_max = x_min + img_size\n",
        "            image = image[x_min:x_max, x_min:x_max, ...]\n",
        "            mask = mask[x_min:x_max, x_min:x_max, ...]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "class Rotate(object):\n",
        "\n",
        "    def __init__(self, angle):\n",
        "        self.angle = angle\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample\n",
        "\n",
        "        angle = np.random.uniform(low=-self.angle, high=self.angle)\n",
        "        image = rotate(image, angle, resize=False, preserve_range=True, mode=\"constant\")\n",
        "        mask = rotate(\n",
        "            mask, angle, resize=False, order=0, preserve_range=True, mode=\"constant\"\n",
        "        )\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "class HorizontalFlip(object):\n",
        "\n",
        "    def __init__(self, flip_prob):\n",
        "        self.flip_prob = flip_prob\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, mask = sample\n",
        "\n",
        "        if np.random.rand() > self.flip_prob:\n",
        "            return image, mask\n",
        "\n",
        "        image = np.fliplr(image).copy()\n",
        "        mask = np.fliplr(mask).copy()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = 1.0\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        assert y_pred.size() == y_true.size()\n",
        "        y_pred = y_pred[:, 0].contiguous().view(-1)\n",
        "        y_true = y_true[:, 0].contiguous().view(-1)\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dsc = (2. * intersection + self.smooth) / (\n",
        "            y_pred.sum() + y_true.sum() + self.smooth\n",
        "        )\n",
        "        return 1. - dsc\n",
        "\n",
        "\n",
        "def log_images(x, y_true, y_pred, channel=1):\n",
        "    images = []\n",
        "    x_np = x[:, channel].cpu().numpy()\n",
        "    y_true_np = y_true[:, 0].cpu().numpy()\n",
        "    y_pred_np = y_pred[:, 0].cpu().numpy()\n",
        "    for i in range(x_np.shape[0]):\n",
        "        image = gray2rgb(np.squeeze(x_np[i]))\n",
        "        image = outline(image, y_pred_np[i], color=[255, 0, 0])\n",
        "        image = outline(image, y_true_np[i], color=[0, 255, 0])\n",
        "        images.append(image)\n",
        "    return images\n",
        "\n",
        "\n",
        "def gray2rgb(image):\n",
        "    w, h = image.shape\n",
        "    image += np.abs(np.min(image))\n",
        "    image_max = np.abs(np.max(image))\n",
        "    if image_max > 0:\n",
        "        image /= image_max\n",
        "    ret = np.empty((w, h, 3), dtype=np.uint8)\n",
        "    ret[:, :, 2] = ret[:, :, 1] = ret[:, :, 0] = image * 255\n",
        "    return ret\n",
        "\n",
        "\n",
        "def outline(image, mask, color):\n",
        "    mask = np.round(mask)\n",
        "    yy, xx = np.nonzero(mask)\n",
        "    for y, x in zip(yy, xx):\n",
        "        if 0.0 < np.mean(mask[max(0, y - 1) : y + 2, max(0, x - 1) : x + 2]) < 1.0:\n",
        "            image[max(0, y) : y + 1, max(0, x) : x + 1] = color\n",
        "    return image\n",
        "\n",
        "\n",
        "def data_loaders(batch_size, workers, image_size, aug_scale, aug_angle):\n",
        "    dataset_train, dataset_valid = datasets(\"./dataset\", image_size, aug_scale, aug_angle)\n",
        "\n",
        "    def worker_init(worker_id):\n",
        "        np.random.seed(42 + worker_id)\n",
        "\n",
        "    loader_train = DataLoader(\n",
        "        dataset_train,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=workers,\n",
        "        worker_init_fn=worker_init,\n",
        "    )\n",
        "    loader_valid = DataLoader(\n",
        "        dataset_valid,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=False,\n",
        "        num_workers=workers,\n",
        "        worker_init_fn=worker_init,\n",
        "    )\n",
        "\n",
        "    return loader_train, loader_valid\n",
        "\n",
        "\n",
        "def datasets(images, image_size, aug_scale, aug_angle):\n",
        "    train = BrainSegmentationDataset(\n",
        "        images_dir=images,\n",
        "        subset=\"train\",\n",
        "        image_size=image_size,\n",
        "        # transform=transforms(scale=aug_scale, angle=aug_angle, flip_prob=0.5),\n",
        "        transform=None,\n",
        "\n",
        "    )\n",
        "    valid = BrainSegmentationDataset(\n",
        "        images_dir=images,\n",
        "        subset=\"validation\",\n",
        "        image_size=image_size,\n",
        "        random_sampling=False,\n",
        "    )\n",
        "    return train, valid\n",
        "\n",
        "\n",
        "def dsc(y_pred, y_true):\n",
        "    y_pred = np.round(y_pred).astype(int)\n",
        "    y_true = np.round(y_true).astype(int)\n",
        "    return np.sum(y_pred[y_true == 1]) * 2.0 / (np.sum(y_pred) + np.sum(y_true))\n",
        "\n",
        "\n",
        "def dsc_distribution(volumes):\n",
        "    dsc_dict = {}\n",
        "    for p in volumes:\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        dsc_dict[p] = dsc(y_pred, y_true)\n",
        "    return dsc_dict\n",
        "\n",
        "\n",
        "def dsc_per_volume(validation_pred, validation_true, patient_slice_index):\n",
        "    dsc_list = []\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        y_pred = np.array(validation_pred[index : index + num_slices[p]])\n",
        "        y_true = np.array(validation_true[index : index + num_slices[p]])\n",
        "        dsc_list.append(dsc(y_pred, y_true))\n",
        "        index += num_slices[p]\n",
        "    return dsc_list\n",
        "\n",
        "\n",
        "def postprocess_per_volume(\n",
        "    input_list, pred_list, true_list, patient_slice_index, patients\n",
        "):\n",
        "    volumes = {}\n",
        "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
        "    index = 0\n",
        "    for p in range(len(num_slices)):\n",
        "        volume_in = np.array(input_list[index : index + num_slices[p]])\n",
        "        volume_pred = np.round(\n",
        "            np.array(pred_list[index : index + num_slices[p]])\n",
        "        ).astype(int)\n",
        "        volume_true = np.array(true_list[index : index + num_slices[p]])\n",
        "        volumes[patients[p]] = (volume_in, volume_pred, volume_true)\n",
        "        index += num_slices[p]\n",
        "    return volumes\n",
        "\n",
        "\n",
        "def log_loss_summary(loss, step, prefix=\"\"):\n",
        "    print(\"epoch {} | {}: {}\".format(step + 1, prefix + \"loss\", np.mean(loss)))\n",
        "\n",
        "def log_scalar_summary(tag, value, step):\n",
        "    print(\"epoch {} | {}: {}\".format(step + 1, tag, value))\n",
        "\n",
        "\n",
        "def plot_dsc(dsc_dist):\n",
        "    y_positions = np.arange(len(dsc_dist))\n",
        "    dsc_dist = sorted(dsc_dist.items(), key=lambda x: x[1])\n",
        "    values = [x[1] for x in dsc_dist]\n",
        "    labels = [x[0] for x in dsc_dist]\n",
        "    labels = [\"_\".join(l.split(\"_\")[1:-1]) for l in labels]\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    canvas = FigureCanvasAgg(fig)\n",
        "    plt.barh(y_positions, values, align=\"center\", color=\"skyblue\")\n",
        "    plt.yticks(y_positions, labels)\n",
        "    plt.xticks(np.arange(0.0, 1.0, 0.1))\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.gca().axvline(np.mean(values), color=\"tomato\", linewidth=2)\n",
        "    plt.gca().axvline(np.median(values), color=\"forestgreen\", linewidth=2)\n",
        "    plt.xlabel(\"Dice coefficient\", fontsize=\"x-large\")\n",
        "    plt.gca().xaxis.grid(color=\"silver\", alpha=0.5, linestyle=\"--\", linewidth=1)\n",
        "    plt.tight_layout()\n",
        "    canvas.draw()\n",
        "    plt.close()\n",
        "    s, (width, height) = canvas.print_to_buffer()\n",
        "    return np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
        "\n",
        "# TODO: Use basic parameters below for original UNet\n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "lr = 0.01\n",
        "workers = 2\n",
        "weights = \"./\"\n",
        "image_size = 224\n",
        "aug_scale = 0.05\n",
        "aug_angle = 15\n",
        "\n",
        "\n",
        "def train_validate():\n",
        "    device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda:0\")\n",
        "    \n",
        "    # TODO: Get rid of data augmentation code in loader\n",
        "    loader_train, loader_valid = data_loaders(batch_size, workers, image_size, aug_scale, aug_angle)\n",
        "    loaders = {\"train\": loader_train, \"valid\": loader_valid}\n",
        "    \n",
        "    unet = UNet(in_channels=BrainSegmentationDataset.in_channels, out_channels=BrainSegmentationDataset.out_channels)\n",
        "    unet.to(device)\n",
        "    \n",
        "    # TODO: Use basic loss function in original UNet\n",
        "    # dsc_loss = DiceLoss()\n",
        "    dsc_loss = nn.CrossEntropyLoss();\n",
        "    best_validation_dsc = 0.0\n",
        "    \n",
        "    # TODO: Use basic optimizer in original UNet\n",
        "    # optimizer = optim.Adam(unet.parameters(), lr=lr)\n",
        "    optimizer = optim.SGD(unet.parameters(), lr=lr, momentum=0.99)\n",
        "    \n",
        "    loss_train = []\n",
        "    loss_valid = []\n",
        "    \n",
        "    step = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for phase in [\"train\", \"valid\"]:\n",
        "            if phase == \"train\":\n",
        "                unet.train()\n",
        "            else:\n",
        "                unet.eval()\n",
        "    \n",
        "            validation_pred = []\n",
        "            validation_true = []\n",
        "    \n",
        "            for i, data in enumerate(loaders[phase]):\n",
        "                if phase == \"train\":\n",
        "                    step += 1\n",
        "    \n",
        "                x, y_true = data\n",
        "                x, y_true = x.to(device), y_true.to(device)\n",
        "    \n",
        "                optimizer.zero_grad()\n",
        "    \n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    y_pred = unet(x)\n",
        "    \n",
        "                    loss = dsc_loss(y_pred, y_true)\n",
        "    \n",
        "                    if phase == \"valid\":\n",
        "                        loss_valid.append(loss.item())\n",
        "                        y_pred_np = y_pred.detach().cpu().numpy()\n",
        "                        validation_pred.extend(\n",
        "                            [y_pred_np[s] for s in range(y_pred_np.shape[0])]\n",
        "                        )\n",
        "                        y_true_np = y_true.detach().cpu().numpy()\n",
        "                        validation_true.extend(\n",
        "                            [y_true_np[s] for s in range(y_true_np.shape[0])]\n",
        "                        )\n",
        "                        \n",
        "                    if phase == \"train\":\n",
        "                        loss_train.append(loss.item())\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "    \n",
        "            if phase == \"train\":\n",
        "                log_loss_summary(loss_train, epoch)\n",
        "                loss_train = []\n",
        "\n",
        "            if phase == \"valid\":\n",
        "                log_loss_summary(loss_valid, epoch, prefix=\"val_\")\n",
        "                mean_dsc = np.mean(\n",
        "                    dsc_per_volume(\n",
        "                        validation_pred,\n",
        "                        validation_true,\n",
        "                        loader_valid.dataset.patient_slice_index,\n",
        "                    )\n",
        "                )\n",
        "                log_scalar_summary(\"val_dsc\", mean_dsc, epoch)\n",
        "                if mean_dsc > best_validation_dsc:\n",
        "                    best_validation_dsc = mean_dsc\n",
        "                    torch.save(unet.state_dict(), os.path.join(weights, \"unet.pt\"))\n",
        "                loss_valid = []\n",
        "    \n",
        "    print(\"\\nBest validation mean DSC: {:4f}\\n\".format(best_validation_dsc))\n",
        "    \n",
        "    state_dict = torch.load(os.path.join(weights, \"unet.pt\"))\n",
        "    unet.load_state_dict(state_dict)\n",
        "    unet.eval()\n",
        "    \n",
        "    input_list = []\n",
        "    pred_list = []\n",
        "    true_list = []\n",
        "    \n",
        "    for i, data in enumerate(loader_valid):\n",
        "        x, y_true = data\n",
        "        x, y_true = x.to(device), y_true.to(device)\n",
        "        with torch.set_grad_enabled(False):\n",
        "            y_pred = unet(x)\n",
        "            y_pred_np = y_pred.detach().cpu().numpy()\n",
        "            pred_list.extend([y_pred_np[s] for s in range(y_pred_np.shape[0])])\n",
        "            y_true_np = y_true.detach().cpu().numpy()\n",
        "            true_list.extend([y_true_np[s] for s in range(y_true_np.shape[0])])\n",
        "            x_np = x.detach().cpu().numpy()\n",
        "            input_list.extend([x_np[s] for s in range(x_np.shape[0])])\n",
        "            \n",
        "    volumes = postprocess_per_volume(\n",
        "        input_list,\n",
        "        pred_list,\n",
        "        true_list,\n",
        "        loader_valid.dataset.patient_slice_index,\n",
        "        loader_valid.dataset.patients,\n",
        "    )\n",
        "    \n",
        "    dsc_dist = dsc_distribution(volumes)\n",
        "\n",
        "    dsc_dist_plot = plot_dsc(dsc_dist)\n",
        "    imsave(\"./dsc.png\", dsc_dist_plot)\n",
        "\n",
        "    for p in volumes:\n",
        "        x = volumes[p][0]\n",
        "        y_pred = volumes[p][1]\n",
        "        y_true = volumes[p][2]\n",
        "        for s in range(x.shape[0]):\n",
        "            image = gray2rgb(x[s, 1])  # channel 1 is for FLAIR\n",
        "            image = outline(image, y_pred[s, 0], color=[255, 0, 0])\n",
        "            image = outline(image, y_true[s, 0], color=[0, 255, 0])\n",
        "            filename = \"{}-{}.png\".format(p, str(s).zfill(2))\n",
        "            filepath = os.path.join(\"./\", filename)\n",
        "            imsave(filepath, image)\n",
        "\n",
        "\n",
        "train_validate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading train images...\n",
            "Volume: 0 Patient ID: None Images: 0 Masks: 0\n",
            "Volume: 1 Patient ID: TCGA_CS_4941_19960909 Images: 23 Masks: 23\n",
            "Volume: 2 Patient ID: TCGA_CS_4942_19970222 Images: 20 Masks: 20\n",
            "Volume: 3 Patient ID: TCGA_CS_4943_20000902 Images: 20 Masks: 20\n",
            "Volume: 4 Patient ID: TCGA_CS_4944_20010208 Images: 20 Masks: 20\n",
            "Volume: 5 Patient ID: TCGA_CS_5393_19990606 Images: 20 Masks: 20\n",
            "Volume: 6 Patient ID: TCGA_CS_5395_19981004 Images: 20 Masks: 20\n",
            "Volume: 7 Patient ID: TCGA_CS_5396_20010302 Images: 24 Masks: 24\n",
            "Volume: 8 Patient ID: TCGA_CS_5397_20010315 Images: 22 Masks: 22\n",
            "Volume: 9 Patient ID: TCGA_CS_6186_20000601 Images: 25 Masks: 25\n",
            "Volume: 10 Patient ID: TCGA_CS_6188_20010812 Images: 24 Masks: 24\n",
            "Volume: 11 Patient ID: TCGA_CS_6290_20000917 Images: 20 Masks: 20\n",
            "Volume: 12 Patient ID: TCGA_CS_6665_20010817 Images: 24 Masks: 24\n",
            "Volume: 13 Patient ID: TCGA_CS_6666_20011109 Images: 26 Masks: 26\n",
            "Volume: 14 Patient ID: TCGA_CS_6667_20011105 Images: 20 Masks: 20\n",
            "Volume: 15 Patient ID: TCGA_CS_6668_20011025 Images: 28 Masks: 28\n",
            "Volume: 16 Patient ID: TCGA_CS_6669_20020102 Images: 22 Masks: 22\n",
            "Volume: 17 Patient ID: TCGA_DU_5849_19950405 Images: 38 Masks: 38\n",
            "Volume: 18 Patient ID: TCGA_DU_5851_19950428 Images: 40 Masks: 40\n",
            "Volume: 19 Patient ID: TCGA_DU_5852_19950709 Images: 36 Masks: 36\n",
            "Volume: 20 Patient ID: TCGA_DU_5853_19950823 Images: 36 Masks: 36\n",
            "Volume: 21 Patient ID: TCGA_DU_5854_19951104 Images: 36 Masks: 36\n",
            "Volume: 22 Patient ID: TCGA_DU_5855_19951217 Images: 26 Masks: 26\n",
            "Volume: 23 Patient ID: TCGA_DU_5871_19941206 Images: 36 Masks: 36\n",
            "Volume: 24 Patient ID: TCGA_DU_5872_19950223 Images: 71 Masks: 71\n",
            "Volume: 25 Patient ID: TCGA_DU_5874_19950510 Images: 38 Masks: 38\n",
            "Volume: 26 Patient ID: TCGA_DU_6399_19830416 Images: 53 Masks: 53\n",
            "Volume: 27 Patient ID: TCGA_DU_6400_19830518 Images: 57 Masks: 57\n",
            "Volume: 28 Patient ID: TCGA_DU_6401_19831001 Images: 51 Masks: 51\n",
            "Volume: 29 Patient ID: TCGA_DU_6404_19850629 Images: 53 Masks: 53\n",
            "Volume: 30 Patient ID: TCGA_DU_6405_19851005 Images: 60 Masks: 60\n",
            "Volume: 31 Patient ID: TCGA_DU_6407_19860514 Images: 58 Masks: 58\n",
            "Volume: 32 Patient ID: TCGA_DU_6408_19860521 Images: 56 Masks: 56\n",
            "Volume: 33 Patient ID: TCGA_DU_7008_19830723 Images: 52 Masks: 52\n",
            "Volume: 34 Patient ID: TCGA_DU_7010_19860307 Images: 58 Masks: 58\n",
            "Volume: 35 Patient ID: TCGA_DU_7013_19860523 Images: 49 Masks: 49\n",
            "Volume: 36 Patient ID: TCGA_DU_7014_19860618 Images: 60 Masks: 60\n",
            "Volume: 37 Patient ID: TCGA_DU_7018_19911220 Images: 36 Masks: 36\n",
            "Volume: 38 Patient ID: TCGA_DU_7019_19940908 Images: 38 Masks: 38\n",
            "Volume: 39 Patient ID: TCGA_DU_7294_19890104 Images: 34 Masks: 34\n",
            "Volume: 40 Patient ID: TCGA_DU_7298_19910324 Images: 32 Masks: 32\n",
            "Volume: 41 Patient ID: TCGA_DU_7299_19910417 Images: 36 Masks: 36\n",
            "Volume: 42 Patient ID: TCGA_DU_7300_19910814 Images: 36 Masks: 36\n",
            "Volume: 43 Patient ID: TCGA_DU_7301_19911112 Images: 35 Masks: 35\n",
            "Volume: 44 Patient ID: TCGA_DU_7302_19911203 Images: 36 Masks: 36\n",
            "Volume: 45 Patient ID: TCGA_DU_7304_19930325 Images: 36 Masks: 36\n",
            "Volume: 46 Patient ID: TCGA_DU_7306_19930512 Images: 42 Masks: 42\n",
            "Volume: 47 Patient ID: TCGA_DU_7309_19960831 Images: 40 Masks: 40\n",
            "Volume: 48 Patient ID: TCGA_DU_8162_19961029 Images: 37 Masks: 37\n",
            "Volume: 49 Patient ID: TCGA_DU_8163_19961119 Images: 37 Masks: 37\n",
            "Volume: 50 Patient ID: TCGA_DU_8164_19970111 Images: 37 Masks: 37\n",
            "Volume: 51 Patient ID: TCGA_DU_8165_19970205 Images: 36 Masks: 36\n",
            "Volume: 52 Patient ID: TCGA_DU_8166_19970322 Images: 36 Masks: 36\n",
            "Volume: 53 Patient ID: TCGA_DU_8167_19970402 Images: 37 Masks: 37\n",
            "Volume: 54 Patient ID: TCGA_DU_8168_19970503 Images: 36 Masks: 36\n",
            "Volume: 55 Patient ID: TCGA_DU_A5TP_19970614 Images: 38 Masks: 38\n",
            "Volume: 56 Patient ID: TCGA_DU_A5TR_19970726 Images: 38 Masks: 38\n",
            "Volume: 57 Patient ID: TCGA_DU_A5TS_19970726 Images: 35 Masks: 35\n",
            "Volume: 58 Patient ID: TCGA_DU_A5TT_19980318 Images: 56 Masks: 56\n",
            "Volume: 59 Patient ID: TCGA_DU_A5TU_19980312 Images: 23 Masks: 23\n",
            "Volume: 60 Patient ID: TCGA_DU_A5TW_19980228 Images: 31 Masks: 31\n",
            "Volume: 61 Patient ID: TCGA_DU_A5TY_19970709 Images: 36 Masks: 36\n",
            "Volume: 62 Patient ID: TCGA_EZ_7264_20010816 Images: 24 Masks: 24\n",
            "Volume: 63 Patient ID: TCGA_FG_5962_20000626 Images: 51 Masks: 51\n",
            "Volume: 64 Patient ID: TCGA_FG_5964_20010511 Images: 26 Masks: 26\n",
            "Volume: 65 Patient ID: TCGA_FG_6688_20020215 Images: 36 Masks: 36\n",
            "Volume: 66 Patient ID: TCGA_FG_6689_20020326 Images: 48 Masks: 48\n",
            "Volume: 67 Patient ID: TCGA_FG_6690_20020226 Images: 60 Masks: 60\n",
            "Volume: 68 Patient ID: TCGA_FG_6691_20020405 Images: 48 Masks: 48\n",
            "Volume: 69 Patient ID: TCGA_FG_6692_20020606 Images: 26 Masks: 26\n",
            "Volume: 70 Patient ID: TCGA_FG_7634_20000128 Images: 27 Masks: 27\n",
            "Volume: 71 Patient ID: TCGA_FG_7637_20000922 Images: 51 Masks: 51\n",
            "Volume: 72 Patient ID: TCGA_FG_7643_20021104 Images: 48 Masks: 48\n",
            "Volume: 73 Patient ID: TCGA_FG_8189_20030516 Images: 60 Masks: 60\n",
            "Volume: 74 Patient ID: TCGA_FG_A4MT_20020212 Images: 50 Masks: 50\n",
            "Volume: 75 Patient ID: TCGA_FG_A4MU_20030903 Images: 36 Masks: 36\n",
            "Volume: 76 Patient ID: TCGA_FG_A60K_20040224 Images: 73 Masks: 73\n",
            "Volume: 77 Patient ID: TCGA_HT_7473_19970826 Images: 32 Masks: 32\n",
            "Volume: 78 Patient ID: TCGA_HT_7475_19970918 Images: 32 Masks: 32\n",
            "Volume: 79 Patient ID: TCGA_HT_7602_19951103 Images: 20 Masks: 20\n",
            "Volume: 80 Patient ID: TCGA_HT_7605_19950916 Images: 32 Masks: 32\n",
            "Volume: 81 Patient ID: TCGA_HT_7608_19940304 Images: 28 Masks: 28\n",
            "Volume: 82 Patient ID: TCGA_HT_7616_19940813 Images: 30 Masks: 30\n",
            "Volume: 83 Patient ID: TCGA_HT_7680_19970202 Images: 21 Masks: 21\n",
            "Volume: 84 Patient ID: TCGA_HT_7684_19950816 Images: 25 Masks: 25\n",
            "Volume: 85 Patient ID: TCGA_HT_7686_19950629 Images: 20 Masks: 20\n",
            "Volume: 86 Patient ID: TCGA_HT_7690_19960312 Images: 24 Masks: 24\n",
            "Volume: 87 Patient ID: TCGA_HT_7692_19960724 Images: 20 Masks: 20\n",
            "Volume: 88 Patient ID: TCGA_HT_7693_19950520 Images: 21 Masks: 21\n",
            "Volume: 89 Patient ID: TCGA_HT_7694_19950404 Images: 21 Masks: 21\n",
            "Volume: 90 Patient ID: TCGA_HT_7855_19951020 Images: 21 Masks: 21\n",
            "Volume: 91 Patient ID: TCGA_HT_7856_19950831 Images: 32 Masks: 32\n",
            "Volume: 92 Patient ID: TCGA_HT_7860_19960513 Images: 22 Masks: 22\n",
            "Volume: 93 Patient ID: TCGA_HT_7874_19950902 Images: 20 Masks: 20\n",
            "Volume: 94 Patient ID: TCGA_HT_7877_19980917 Images: 30 Masks: 30\n",
            "Volume: 95 Patient ID: TCGA_HT_7879_19981009 Images: 28 Masks: 28\n",
            "Volume: 96 Patient ID: TCGA_HT_7881_19981015 Images: 80 Masks: 80\n",
            "Volume: 97 Patient ID: TCGA_HT_7882_19970125 Images: 32 Masks: 32\n",
            "Volume: 98 Patient ID: TCGA_HT_7884_19980913 Images: 20 Masks: 20\n",
            "Volume: 99 Patient ID: TCGA_HT_8018_19970411 Images: 20 Masks: 20\n",
            "Volume: 100 Patient ID: TCGA_HT_8105_19980826 Images: 32 Masks: 32\n",
            "Volume: 101 Patient ID: TCGA_HT_8106_19970727 Images: 22 Masks: 22\n",
            "Volume: 102 Patient ID: TCGA_HT_8107_19980708 Images: 20 Masks: 20\n",
            "Volume: 103 Patient ID: TCGA_HT_8111_19980330 Images: 22 Masks: 22\n",
            "Volume: 104 Patient ID: TCGA_HT_8113_19930809 Images: 21 Masks: 21\n",
            "Volume: 105 Patient ID: TCGA_HT_8114_19981030 Images: 22 Masks: 22\n",
            "Volume: 106 Patient ID: TCGA_HT_8563_19981209 Images: 23 Masks: 23\n",
            "Volume: 107 Patient ID: TCGA_HT_A5RC_19990831 Images: 32 Masks: 32\n",
            "Volume: 108 Patient ID: TCGA_HT_A616_19991226 Images: 28 Masks: 28\n",
            "Volume: 109 Patient ID: TCGA_HT_A61A_20000127 Images: 88 Masks: 88\n",
            "Volume: 110 Patient ID: TCGA_HT_A61B_19991127 Images: 88 Masks: 88\n",
            "preprocessing train volumes...\n",
            "cropping train volumes...\n",
            "padding train volumes...\n",
            "resizing train volumes...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32wRxAUJRifv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}